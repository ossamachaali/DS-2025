

# Compte Rendu : Classification des Vins avec k-NN

## Introduction

Ce rapport présente une implémentation du classifieur **k-Nearest
Neighbors (k-NN)** appliqué à la classification de la qualité des vins
blancs.\
L'objectif est de prédire si un vin est **de bonne** ou **de mauvaise
qualité** à partir de leurs caractéristiques physico-chimiques.

------------------------------------------------------------------------

## 1. Description du Projet

La classification choisie est binaire :

-   **Classe 0** : Vins de mauvaise qualité (qualité ≤ 5)\
-   **Classe 1** : Vins de bonne qualité (qualité \> 5)

Le dataset provient du **Machine Learning Repository de l'UCI** et
contient **11 caractéristiques physico-chimiques**.

------------------------------------------------------------------------

## 2. Préparation des Données

### 2.1 Chargement et Exploration

-   **X** : 11 variables (acidité, alcool, densité, chlorures, etc.)\
-   **Y** : Score de qualité du vin (transformé en classification
    binaire)

### 2.2 Partitionnement des Données

  ------------------------------------------------------------------------
  Ensemble             Proportion               Description
  -------------------- ------------------------ --------------------------
  Xa, Ya (train)       33.3%                    Apprentissage

  Xv, Yv (validation)  33.3% des données        Recherche du meilleur k
                       restantes                

  Xt, Yt (test)        33.3%                    Évaluation finale
  ------------------------------------------------------------------------

La stratification permet de conserver le même équilibre entre les
classes dans chaque sous-ensemble.

------------------------------------------------------------------------

## 3. Classification avec k-NN

### 3.1 Principe

1.  Calcul des distances euclidiennes\
2.  Sélection des k plus proches voisins\
3.  Vote majoritaire

### 3.2 Taux d'erreur

Le taux d'erreur correspond à la proportion de prédictions incorrectes.

### 3.3 Tests de k (de 1 à 40)

-   L'erreur d'apprentissage diminue lorsque k augmente.\
-   L'erreur de validation forme une courbe en U.\
-   Surapprentissage observé pour k = 1 ou k = 3.

------------------------------------------------------------------------

## 4. Analyse du Surapprentissage

-   **k petit** : variance élevée, surapprentissage.\
-   **k grand** : biais élevé, perte de précision.\
-   **k optimal** : celui minimisant l'erreur sur l'ensemble de
    validation.

------------------------------------------------------------------------

## 5. Normalisation

### Pourquoi normaliser ?

Le k-NN utilise des distances : les variables non normalisées dominent
le calcul, créant un biais.

### Méthode utilisée : Standardisation (Z-score)

    X_norm = (X - moyenne) / écart_type

Réalisée avec **StandardScaler** de scikit-learn.

Important : le scaling est ajusté uniquement sur **Xa** (train), puis
appliqué à Xv et Xt.

------------------------------------------------------------------------

## 6. Résultats Résumés

  Aspect                   Information
  ------------------------ ----------------------------
  Algorithme               k-NN
  Caractéristiques         11
  Classes                  2
  Objectif                 Classification binaire
  Recherche du k optimal   de 1 à 40
  Métrique                 Taux d'erreur (validation)
  Normalisation            Z-score (indispensable)

------------------------------------------------------------------------

## 7. Conclusions

-   Le k-NN surapprend pour les valeurs faibles de k.\
-   Le choix du k optimal se fait via l'erreur de validation.\
-   La normalisation est essentielle pour de bonnes performances.\
-   Les résultats dépendent fortement de la séparation
    train/validation/test.

------------------------------------------------------------------------

## Recommandations

-   Utiliser une validation croisée (k-fold) pour réduire l'impact de la
    partition.\
-   Tester d'autres mesures de distance (Manhattan, Minkowski).\
-   Comparer avec d'autres modèles :
    -   Régression Logistique\
    -   SVM\
    -   Random Forest\
-   Étudier l'importance des variables (permutation importance).

------------------------------------------------------------------------

Document généré automatiquement.
