
# Compte Rendu : Classification des Vins avec k-NN

## Introduction
Ce rapport présente une implémentation du classifieur **k-Nearest Neighbors (k-NN)** appliqué à la classification de la qualité des vins blancs.  
L'objectif est de prédire si un vin est **de bonne** ou **de mauvaise qualité** à partir de leurs caractéristiques physico-chimiques.

## 1. Description du Projet
La classification est binaire :
- **Classe 0** : Vins de mauvaise qualité (qualité ≤ 5)
- **Classe 1** : Vins de bonne qualité (qualité > 5)

Le dataset provient du **Machine Learning Repository de l'UCI** et contient **11 caractéristiques physico-chimiques**.

## 2. Préparation des Données

### 2.1 Chargement et Exploration
- **X** : 11 variables (acidité, alcool, densité, chlorures, etc.)
- **Y** : Score de qualité du vin (transformé en classification binaire)

### 2.2 Partitionnement des Données
| Ensemble | Proportion | Description |
|----------|------------|-------------|
| **Xa, Ya** (train) | 33.3% | Apprentissage |
| **Xv, Yv** (validation) | 33.3% des données restantes | Recherche du meilleur k |
| **Xt, Yt** (test) | 33.3% | Évaluation finale |

La stratification maintient l’équilibre entre les classes.

## 3. Classification avec k-NN

### 3.1 Principe
1. Calcul des distances euclidiennes  
2. Sélection des **k plus proches voisins**  
3. Vote majoritaire  

### 3.2 Taux d’erreur
Erreur = proportion de prédictions incorrectes.

### 3.3 Tests de k (1 → 40)
- Erreur d’apprentissage ↓ avec k  
- Erreur de validation → forme en U  
- Surapprentissage marqué pour k = 1, 3  

## 4. Analyse du Surapprentissage
- **k petit** → Variance élevée, overfitting  
- **k grand** → Biais fort  
- **k optimal** = celui minimisant l’erreur de validation  

## 5. Normalisation

### Justification
k-NN utilise les distances → variables non normalisées dominent.

### Standardisation
X_norm = (X - μ) / σ  
Scikit-learn : StandardScaler  
⚠ Fit sur Xa uniquement (pas sur validation/test)

## 6. Résultats Résumés
| Aspect | Valeur |
|--------|--------|
| Algorithme | k-NN |
| Caractéristiques | 11 |
| Classes | 2 |
| k optimal | Minimisation erreur validation |
| Normalisation | Z-score |
| Recherche de k | 1 → 40 |

## 7. Conclusions
- k-NN surapprend pour k petit  
- Validation = choix optimal  
- Normalisation essentielle  
- Sensibilité à la partition (train/val/test)

## Recommandations
- Validation croisée k-fold  
- Tester distances alternatives (Manhattan, Minkowski)  
- Comparer avec SVM, Régression Logistique, Random Forest  
- Étudier l’importance des variables
