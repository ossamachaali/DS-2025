# -*- coding: utf-8 -*-
"""Chaali Ossama .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14xDPt-JRmZJ6kCqNDZ5B8zn-snUMvzl5
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("salmaemadali/iris-3")

print("Path to dataset files:", path)

import pandas as pd

# Read the Iris dataset into a DataFrame
iris = pd.read_csv(f"{path}/Iris.csv")
iris

iris.head(20) #show the first 20 rows from the dataset

iris.info()  #checking if there is any inconsistency in the dataset
#as we see there are no null values in the dataset, so the data can be processed

iris.describe()

iris.columns

iris.Species.unique()

#The Iris dataset contains only three types of flowers.

import matplotlib.pyplot as plt

fig = iris[iris.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='Setosa')
iris[iris.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)
iris[iris.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)
fig.set_xlabel("Sepal Length")
fig.set_ylabel("Sepal Width")
fig.set_title("Sepal Length VS Width")
fig=plt.gcf()
fig.set_size_inches(10,6)
plt.show()

fig = iris[iris.Species=='Iris-setosa'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='orange', label='Setosa')
iris[iris.Species=='Iris-versicolor'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='blue', label='versicolor',ax=fig)
iris[iris.Species=='Iris-virginica'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='green', label='virginica', ax=fig)
fig.set_xlabel("Petal Length")
fig.set_ylabel("Petal Width")
fig.set_title(" Petal Length VS Width")
fig=plt.gcf()
fig.set_size_inches(10,6)
plt.show()

iris.hist(edgecolor='black', linewidth=1.2)
fig=plt.gcf()
fig.set_size_inches(12,6)
plt.show()

import seaborn as sns

plt.figure(figsize=(15,10))
plt.subplot(2,2,1)
sns.violinplot(x='Species',y='PetalLengthCm',data=iris)
plt.subplot(2,2,2)
sns.violinplot(x='Species',y='PetalWidthCm',data=iris)
plt.subplot(2,2,3)
sns.violinplot(x='Species',y='SepalLengthCm',data=iris)
plt.subplot(2,2,4)
sns.violinplot(x='Species',y='SepalWidthCm',data=iris)

from sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm
from sklearn.model_selection import train_test_split #to split the dataset for training and testing
from sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours
from sklearn import svm  #for Support Vector Machine (SVM) Algorithm
from sklearn import metrics #for checking the model accuracy
from sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm

iris.shape

plt.figure(figsize=(7,4))
sns.heatmap(iris.drop('Species', axis=1).corr(), annot=True, cmap='cubehelix_r') # Drop the non-numeric 'Species' column before calculating correlation
plt.show()

train, test = train_test_split(iris, test_size = 0.3)# in this our main data is split into train and test
# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%
print(train.shape)
print(test.shape)

train_X = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]# taking the training data features
train_y=train.Species# output of our training data
test_X= test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] # taking test data features
test_y =test.Species   #output value of test data

train_X.head(2)

test_X.head(2)

train_y.head()  ##output of the training data

model = svm.SVC() #select the algorithm
model.fit(train_X,train_y) # we train the algorithm with the training data and the training output
prediction=model.predict(test_X) #now we pass the testing data to the trained algorithm
print('The accuracy of the SVM is:',metrics.accuracy_score(prediction,test_y))#now we check the accuracy of the algorithm.
#we pass the predicted output by the model and the actual output

model = LogisticRegression()
model.fit(train_X,train_y)
prediction=model.predict(test_X)
print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,test_y))

model=DecisionTreeClassifier()
model.fit(train_X,train_y)
prediction=model.predict(test_X)
print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,test_y))

model=KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class
model.fit(train_X,train_y)
prediction=model.predict(test_X)
print('The accuracy of the KNN is',metrics.accuracy_score(prediction,test_y))

# 1. Predict on training and validation sets

# Using the SVM model as an example for best_model
# And mapping existing train/test data to the variables expected by this block
best_model = svm.SVC() # Re-initialize the SVM model
best_model.fit(train_X, train_y) # Train it with the training data

X_train = train_X
X_valid = test_X
y_train = train_y
y_valid = test_y

y_train_pred = best_model.predict(X_train)
y_valid_pred = best_model.predict(X_valid)

# 2. Calculate classification metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

train_accuracy = accuracy_score(y_train, y_train_pred)
valid_accuracy = accuracy_score(y_valid, y_valid_pred)

# For multiclass, precision, recall, f1-score need an 'average' parameter
# 'weighted' is a common choice that accounts for label imbalance
train_precision = precision_score(y_train, y_train_pred, average='weighted', zero_division=0)
valid_precision = precision_score(y_valid, y_valid_pred, average='weighted', zero_division=0)

train_recall = recall_score(y_train, y_train_pred, average='weighted', zero_division=0)
valid_recall = recall_score(y_valid, y_valid_pred, average='weighted', zero_division=0)

train_f1 = f1_score(y_train, y_train_pred, average='weighted', zero_division=0)
valid_f1 = f1_score(y_valid, y_valid_pred, average='weighted', zero_division=0)


# 3. Display Comparison and Check for Overfitting/Underfitting
print(f"--- Performance Comparison for {best_model.__class__.__name__} ---")
print(f"Accuracy (Train): {train_accuracy:.4f}")
print(f"Accuracy (Validation): {valid_accuracy:.4f}")
print("---------------------------------")
print(f"Precision (Train): {train_precision:.4f}")
print(f"Precision (Validation): {valid_precision:.4f}")
print("---------------------------------")
print(f"Recall (Train): {train_recall:.4f}")
print(f"Recall (Validation): {valid_recall:.4f}")
print("---------------------------------")
print(f"F1-Score (Train): {train_f1:.4f}")
print(f"F1-Score (Validation): {valid_f1:.4f}")

# Overfitting/Underfitting Check (using Accuracy as primary metric for simplicity)
if (train_accuracy - valid_accuracy) > 0.1:
    print("\n Warning: Potential Overfitting!")
    print("The model performs significantly better on training data than on validation data.")
elif valid_accuracy < 0.7:
    print("\n Warning: Potential Underfitting.")
    print("The model is not learning well, even on the validation data.")
else:
    print("\n Performance is good and balanced.")

petal=iris[['PetalLengthCm','PetalWidthCm','Species']]
sepal=iris[['SepalLengthCm','SepalWidthCm','Species']]

train_p,test_p=train_test_split(petal,test_size=0.3,random_state=0)  #petals
train_x_p=train_p[['PetalWidthCm','PetalLengthCm']]
train_y_p=train_p.Species
test_x_p=test_p[['PetalWidthCm','PetalLengthCm']]
test_y_p=test_p.Species


train_s,test_s=train_test_split(sepal,test_size=0.3,random_state=0)  #Sepal
train_x_s=train_s[['SepalWidthCm','SepalLengthCm']]
train_y_s=train_s.Species
test_x_s=test_s[['SepalWidthCm','SepalLengthCm']]
test_y_s=test_s.Species

model=svm.SVC()
model.fit(train_x_p,train_y_p)
prediction=model.predict(test_x_p)
print('The accuracy of the SVM using Petals is:',metrics.accuracy_score(prediction,test_y_p))

model=svm.SVC()
model.fit(train_x_s,train_y_s)
prediction=model.predict(test_x_s)
print('The accuracy of the SVM using Sepal is:',metrics.accuracy_score(prediction,test_y_s))

model = LogisticRegression()
model.fit(train_x_p,train_y_p)
prediction=model.predict(test_x_p)
print('The accuracy of the Logistic Regression using Petals is:',metrics.accuracy_score(prediction,test_y_p))

model.fit(train_x_s,train_y_s)
prediction=model.predict(test_x_s)
print('The accuracy of the Logistic Regression using Sepals is:',metrics.accuracy_score(prediction,test_y_s))

model=DecisionTreeClassifier()
model.fit(train_x_p,train_y_p)
prediction=model.predict(test_x_p)
print('The accuracy of the Decision Tree using Petals is:',metrics.accuracy_score(prediction,test_y_p))

model.fit(train_x_s,train_y_s)
prediction=model.predict(test_x_s)
print('The accuracy of the Decision Tree using Sepals is:',metrics.accuracy_score(prediction,test_y_s))

model=KNeighborsClassifier(n_neighbors=3)
model.fit(train_x_p,train_y_p)
prediction=model.predict(test_x_p)
print('The accuracy of the KNN using Petals is:',metrics.accuracy_score(prediction,test_y_p))

model.fit(train_x_s,train_y_s)
prediction=model.predict(test_x_s)
print('The accuracy of the KNN using Sepals is:',metrics.accuracy_score(prediction,test_y_s))



import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.regplot(x='SepalLengthCm', y='SepalWidthCm', data=iris)
plt.title('Linear Regression: Sepal Length vs Sepal Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.grid(True)
plt.show()